# =============================================================================
# Eclaire Configuration (Docker)
# =============================================================================
# 1. Copy to .env:    cp .env.example .env
# 2. Generate secrets: for i in {1..3}; do openssl rand -hex 32; done
# 3. Start LLM backend
# 4. Start app:       docker compose up -d
# =============================================================================

# -----------------------------------------------------------------------------
# Security Secrets (REQUIRED - generate with: openssl rand -hex 32)
# -----------------------------------------------------------------------------
BETTER_AUTH_SECRET=
MASTER_ENCRYPTION_KEY=
API_KEY_HMAC_KEY_V1=

# -----------------------------------------------------------------------------
# Database Configuration
# -----------------------------------------------------------------------------
# Default: postgres
# Options: sqlite | pglite | postgres
DATABASE_TYPE=postgres

# PostgreSQL settings (defaults work with included postgres container)
#DATABASE_HOST=postgres
#DATABASE_PORT=5432
#DATABASE_USER=eclaire
#DATABASE_PASSWORD=eclaire
#DATABASE_NAME=eclaire

# Or provide a full connection string (overrides individual settings above)
DATABASE_URL=postgresql://eclaire:eclaire@postgres:5432/eclaire

# SQLite/PGlite data directories (only if using sqlite/pglite)
#SQLITE_DATA_DIR=./data/sqlite
#PGLITE_DATA_DIR=./data/pglite

# -----------------------------------------------------------------------------
# Queue Configuration
# -----------------------------------------------------------------------------
# Default: postgres (uses PostgreSQL for job queue)
# Options: sqlite | postgres | redis
#
# Note: sqlite queue requires SERVICE_ROLE=all (single process mode)
# and DATABASE_TYPE=sqlite. QUEUE_BACKEND must match DATABASE_TYPE
# (sqlite with sqlite, postgres with postgres/pglite) unless using redis.
QUEUE_BACKEND=postgres

# -----------------------------------------------------------------------------
# Service Mode
# -----------------------------------------------------------------------------
# Default: all (unified mode - API + workers in single process)
# Options: all | api | worker
#
# - all: Run everything in one process (recommended for simple deployments)
# - api: Run only the HTTP API (workers run separately)
# - worker: Run only queue workers (API runs separately)
#SERVICE_ROLE=all

# -----------------------------------------------------------------------------
# Server
# -----------------------------------------------------------------------------
# Port the app listens on (also used for host port mapping in compose)
#PORT=3000

# -----------------------------------------------------------------------------
# URLs
# -----------------------------------------------------------------------------
# Your public-facing URL (for OAuth callbacks, email links, etc.)
#FRONTEND_URL=http://localhost:3000

# -----------------------------------------------------------------------------
# AI Configuration
# -----------------------------------------------------------------------------
# AI providers and models are configured in config/ai/*.json
# Copy config/ai/providers.json.example to config/ai/providers.json
# and configure your providers.

# AI Provider API Keys (referenced by config/ai/providers.json via ${ENV:VAR})
#OPENAI_API_KEY=sk-...
#ANTHROPIC_API_KEY=sk-ant-...
#OPENROUTER_API_KEY=sk-or-...

# Local LLM server URLs (for Docker, use host.docker.internal to reach host machine)
LLAMA_CPP_BASE_URL=http://host.docker.internal:11500/v1
OLLAMA_BASE_URL=http://host.docker.internal:11434/v1
LM_STUDIO_BASE_URL=http://host.docker.internal:1234/v1
MLX_LM_BASE_URL=http://host.docker.internal:8080/v1
MLX_VLM_BASE_URL=http://host.docker.internal:8080/v1

# -----------------------------------------------------------------------------
# External Services
# -----------------------------------------------------------------------------
# Docling document processing server (runs in compose, no config needed)
# Default: http://docling:5001

# GitHub token (for higher rate limits on GitHub bookmark processing)
#GITHUB_TOKEN=

# Reddit API (for Reddit bookmark processing)
#REDDIT_CLIENT_ID=
#REDDIT_CLIENT_SECRET=

# -----------------------------------------------------------------------------
# OPTIONAL: Performance Tuning
# -----------------------------------------------------------------------------
#LOG_LEVEL=info
#WORKER_CONCURRENCY=5
#AI_TIMEOUT=180000
